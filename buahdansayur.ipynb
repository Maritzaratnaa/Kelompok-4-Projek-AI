{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3173719,"sourceType":"datasetVersion","datasetId":952827}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport os.path\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img,img_to_array","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-17T14:10:20.356808Z","iopub.execute_input":"2025-06-17T14:10:20.357439Z","iopub.status.idle":"2025-06-17T14:10:20.361762Z","shell.execute_reply.started":"2025-06-17T14:10:20.357413Z","shell.execute_reply":"2025-06-17T14:10:20.360992Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"base_path = Path('/kaggle/input/fruit-and-vegetable-image-recognition')\n\ntrain_dir = base_path / 'train'\ntrain_filepaths = list(train_dir.glob('**/*.jpg'))\n\nval_dir = base_path / 'validation'\nval_filepaths = list(val_dir.glob('**/*.jpg'))\n\ntest_dir = base_path / 'test'\ntest_filepaths = list(test_dir.glob('**/*.jpg'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T14:10:20.362928Z","iopub.execute_input":"2025-06-17T14:10:20.363172Z","iopub.status.idle":"2025-06-17T14:10:25.071877Z","shell.execute_reply.started":"2025-06-17T14:10:20.363157Z","shell.execute_reply":"2025-06-17T14:10:25.071328Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def image_processing(filepath):\n    labels = [str(filepath[i]).split(\"/\")[-2] \\\n              for i in range(len(filepath))]\n\n    filepath = pd.Series(filepath, name='Filepath').astype(str)\n    labels = pd.Series(labels, name='Label')\n\n    df = pd.concat([filepath, labels], axis=1)\n    df = df.sample(frac=1).reset_index(drop = True)\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T14:10:25.072809Z","iopub.execute_input":"2025-06-17T14:10:25.073066Z","iopub.status.idle":"2025-06-17T14:10:25.077927Z","shell.execute_reply.started":"2025-06-17T14:10:25.073045Z","shell.execute_reply":"2025-06-17T14:10:25.077150Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_df = image_processing(train_filepaths)\ntest_df = image_processing(test_filepaths)\nval_df = image_processing(val_filepaths)\n\nprint('-- Training set --\\n')\nprint(f'Number of pictures: {train_df.shape[0]}\\n')\nprint(f'Number of different labels: {len(train_df.Label.unique())}\\n')\nprint(f'Labels: {train_df.Label.unique()}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T14:10:25.078763Z","iopub.execute_input":"2025-06-17T14:10:25.079080Z","iopub.status.idle":"2025-06-17T14:10:25.115631Z","shell.execute_reply.started":"2025-06-17T14:10:25.079064Z","shell.execute_reply":"2025-06-17T14:10:25.115091Z"}},"outputs":[{"name":"stdout","text":"-- Training set --\n\nNumber of pictures: 2780\n\nNumber of different labels: 36\n\nLabels: ['pear' 'kiwi' 'paprika' 'cabbage' 'cauliflower' 'tomato' 'bell pepper'\n 'grapes' 'capsicum' 'chilli pepper' 'pineapple' 'spinach' 'cucumber'\n 'beetroot' 'mango' 'soy beans' 'raddish' 'orange' 'banana' 'sweetcorn'\n 'eggplant' 'lettuce' 'peas' 'lemon' 'watermelon' 'sweetpotato' 'corn'\n 'onion' 'apple' 'jalepeno' 'potato' 'turnip' 'carrot' 'ginger'\n 'pomegranate' 'garlic']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n)\n\ntest_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T14:10:25.117328Z","iopub.execute_input":"2025-06-17T14:10:25.117539Z","iopub.status.idle":"2025-06-17T14:10:25.131676Z","shell.execute_reply.started":"2025-06-17T14:10:25.117513Z","shell.execute_reply":"2025-06-17T14:10:25.131014Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"train_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=0,\n    rotation_range=30,\n    zoom_range=0.15,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.15,\n    horizontal_flip=True,\n    fill_mode=\"nearest\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T14:10:25.132488Z","iopub.execute_input":"2025-06-17T14:10:25.132726Z","iopub.status.idle":"2025-06-17T14:10:26.435741Z","shell.execute_reply.started":"2025-06-17T14:10:25.132710Z","shell.execute_reply":"2025-06-17T14:10:26.434996Z"}},"outputs":[{"name":"stdout","text":"Found 2780 validated image filenames belonging to 36 classes.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"val_images = train_generator.flow_from_dataframe(\n    dataframe=val_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=0,\n    rotation_range=30,\n    zoom_range=0.15,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.15,\n    horizontal_flip=True,\n    fill_mode=\"nearest\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T14:10:26.436870Z","iopub.execute_input":"2025-06-17T14:10:26.437184Z","iopub.status.idle":"2025-06-17T14:10:27.109231Z","shell.execute_reply.started":"2025-06-17T14:10:26.437159Z","shell.execute_reply":"2025-06-17T14:10:27.108681Z"}},"outputs":[{"name":"stdout","text":"Found 334 validated image filenames belonging to 36 classes.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"test_images = test_generator.flow_from_dataframe(\n    dataframe=test_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T14:10:27.109889Z","iopub.execute_input":"2025-06-17T14:10:27.110111Z","iopub.status.idle":"2025-06-17T14:10:27.822057Z","shell.execute_reply.started":"2025-06-17T14:10:27.110095Z","shell.execute_reply":"2025-06-17T14:10:27.821380Z"}},"outputs":[{"name":"stdout","text":"Found 334 validated image filenames belonging to 36 classes.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"pretrained_model = tf.keras.applications.MobileNetV2(\n    input_shape=(224, 224, 3),\n    include_top=False,\n    weights='imagenet',\n    pooling='avg'\n)\npretrained_model.trainable = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T14:10:27.822793Z","iopub.execute_input":"2025-06-17T14:10:27.823032Z","iopub.status.idle":"2025-06-17T14:10:28.507890Z","shell.execute_reply.started":"2025-06-17T14:10:27.823015Z","shell.execute_reply":"2025-06-17T14:10:28.507304Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"inputs = pretrained_model.input\n\nx = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\n\noutputs = tf.keras.layers.Dense(36, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model.fit(\n    train_images,\n    validation_data=val_images,\n    batch_size = 32,\n    epochs=10,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=2,\n            restore_best_weights=True\n        )\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T14:10:28.508723Z","iopub.execute_input":"2025-06-17T14:10:28.509030Z","iopub.status.idle":"2025-06-17T14:22:20.019832Z","shell.execute_reply.started":"2025-06-17T14:10:28.509005Z","shell.execute_reply":"2025-06-17T14:22:20.019280Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 894ms/step - accuracy: 0.3446 - loss: 2.5560 - val_accuracy: 0.8533 - val_loss: 0.5112\nEpoch 2/10\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 804ms/step - accuracy: 0.8198 - loss: 0.5801 - val_accuracy: 0.9102 - val_loss: 0.3171\nEpoch 3/10\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 793ms/step - accuracy: 0.8925 - loss: 0.3389 - val_accuracy: 0.9102 - val_loss: 0.2653\nEpoch 4/10\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 793ms/step - accuracy: 0.9351 - loss: 0.2210 - val_accuracy: 0.9222 - val_loss: 0.2342\nEpoch 5/10\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 809ms/step - accuracy: 0.9513 - loss: 0.1620 - val_accuracy: 0.9581 - val_loss: 0.1737\nEpoch 6/10\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 782ms/step - accuracy: 0.9721 - loss: 0.1077 - val_accuracy: 0.9371 - val_loss: 0.1845\nEpoch 7/10\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 782ms/step - accuracy: 0.9764 - loss: 0.0849 - val_accuracy: 0.9671 - val_loss: 0.1653\nEpoch 8/10\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 815ms/step - accuracy: 0.9842 - loss: 0.0585 - val_accuracy: 0.9701 - val_loss: 0.1807\nEpoch 9/10\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 792ms/step - accuracy: 0.9883 - loss: 0.0504 - val_accuracy: 0.9641 - val_loss: 0.1560\nEpoch 10/10\n\u001b[1m87/87\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 792ms/step - accuracy: 0.9911 - loss: 0.0380 - val_accuracy: 0.9401 - val_loss: 0.2187\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"from IPython.display import FileLink\nmodel.save('/kaggle/working/model_buah_sayur.h5')\nFileLink('model_buah_sayur.h5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T14:22:20.021635Z","iopub.execute_input":"2025-06-17T14:22:20.021830Z","iopub.status.idle":"2025-06-17T14:22:20.323991Z","shell.execute_reply.started":"2025-06-17T14:22:20.021816Z","shell.execute_reply":"2025-06-17T14:22:20.323258Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/model_buah_sayur.h5","text/html":"<a href='model_buah_sayur.h5' target='_blank'>model_buah_sayur.h5</a><br>"},"metadata":{}}],"execution_count":22}]}